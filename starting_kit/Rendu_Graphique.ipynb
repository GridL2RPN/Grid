{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Modification de la classe Runner.py pour renvoyer les éléments intérressants </h1>\n",
    "Changement de loop et step dans le return afin de renvoyer les éléments intérressants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypownet.environment import RunEnv\n",
    "from pypownet.agent import Agent\n",
    "import logging\n",
    "import logging.handlers\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "LOG_FILENAME = 'runner.log'\n",
    "\n",
    "\n",
    "class TimestepTimeout(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class RunnerPerso(object):\n",
    "    def __init__(self, environment, agent, render=False, verbose=False, vverbose=False, log_filepath='runner.log'):\n",
    "        # Sanity checks: both environment and agent should inherit resp. RunEnv and Agent\n",
    "        assert isinstance(environment, RunEnv)\n",
    "        assert isinstance(agent, Agent)\n",
    "\n",
    "        # Loggger part\n",
    "        self.logger = logging.getLogger('pypownet')\n",
    "\n",
    "        # Always create a log file for runners\n",
    "        fh = logging.FileHandler(filename=log_filepath, mode='w+')\n",
    "        fh.setLevel(logging.DEBUG)\n",
    "        fh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "        self.logger.addHandler(fh)\n",
    "\n",
    "        if verbose or vverbose:\n",
    "            # create console handler, set level to debug, create formatter\n",
    "            ch = logging.StreamHandler()\n",
    "            ch.setLevel(logging.DEBUG if vverbose and verbose else logging.INFO)\n",
    "            ch.setFormatter(logging.Formatter('%(levelname)s        %(message)s'))\n",
    "            self.ch = ch\n",
    "            # add ch to logger\n",
    "            self.logger.addHandler(ch)\n",
    "            self.logger.setLevel(logging.DEBUG if vverbose else logging.INFO)\n",
    "\n",
    "        self.environment = environment\n",
    "        self.agent = agent\n",
    "        self.verbose = verbose\n",
    "        self.render = render\n",
    "\n",
    "        # First observation given by the environment\n",
    "        self.last_observation = self.environment._get_obs()\n",
    "\n",
    "        self.max_seconds_per_timestep = self.environment.game.get_max_seconds_per_timestep()\n",
    "\n",
    "        if self.render:\n",
    "            self.environment.render()\n",
    "\n",
    "    def step(self):\n",
    "        # Policy inference\n",
    "        # def agent_inference(obs, q):\n",
    "        #     action = self.agent.act(obs)\n",
    "        #     q.put(action)\n",
    "        # q = queue.Queue()\n",
    "        # t = threading.Thread(target=agent_inference, name='AgentActThread', args=(self.last_observation, q))\n",
    "        # t.start()\n",
    "        # t.join(self.max_seconds_per_timestep)\n",
    "        # if t.is_alive():\n",
    "        #     self.logger.warn('\\b\\b\\bTook too much time to compute action for current timestep: allowed at most %s '\n",
    "        #                      'seconds; emulating do-nothing action' % str(self.max_seconds_per_timestep))\n",
    "        #     action = self.environment.action_space.get_do_nothing_action()\n",
    "        # else:\n",
    "        #     action = q.get()\n",
    "\n",
    "        action = self.agent.act(self.last_observation)\n",
    "\n",
    "        # Update the environment with the chosen action\n",
    "        observation, reward_aslist, done, info = self.environment.step(action, do_sum=False)\n",
    "        if done:\n",
    "            self.logger.warn('\\b\\b\\bGAME OVER! Resetting grid... (hint: %s)' % info.text)\n",
    "            observation = self.environment.reset()\n",
    "        elif info:\n",
    "            self.logger.warn(info.text)\n",
    "\n",
    "        reward = sum(reward_aslist)\n",
    "\n",
    "        if self.render:\n",
    "            self.environment.render()\n",
    "\n",
    "        self.last_observation = observation\n",
    "\n",
    "        self.agent.feed_reward(action, observation, reward_aslist)\n",
    "\n",
    "        self.logger.debug('action: %s' % ('[%s]' % ' '.join(list(map(lambda x: str(int(x)), action.as_array())))))\n",
    "        self.logger.debug('reward: {}'.format('[' + ','.join(list(map(str, reward_aslist))) + ']'))\n",
    "        self.logger.debug('done: {}'.format(done))\n",
    "        self.logger.debug('info: {}'.format(info if not info else info.text))\n",
    "        self.logger.debug('observation: \\n%s' % observation.__str__())\n",
    "        \n",
    "        #print(self.environment.game.get_number_elements())\n",
    "\n",
    "        return observation, action, reward\n",
    "\n",
    "    #Dans cette fonction on va rajouter au retour tous les elements que l on souhaite afficher\n",
    "    def loop(self, iterations):\n",
    "        cumul_rew = 0.0\n",
    "        rewListCumul = []\n",
    "        rewList = []\n",
    "        obsList = []\n",
    "        actList = []\n",
    "        for i in range(1, iterations + 1):\n",
    "            if (i%100==0):\n",
    "                print(i)\n",
    "            (obs, act, rew) = self.step()\n",
    "            cumul_rew += rew\n",
    "            rewList.append(rew)\n",
    "            rewListCumul.append(cumul_rew)\n",
    "            obsList.append(obs)\n",
    "            actList.append(act)\n",
    "            self.logger.info(\"step %d/%d - reward: %.2f; cumulative reward: %.2f\" % (i, iterations, rew, cumul_rew))\n",
    "            #print(obs.planned_voltage_productions, '||', obs.planned_reactive_loads , '||', obs.ampere_flows)\n",
    "            #print(obs.ampere_flows, '\\n', obs.thermal_limits)\n",
    "            \n",
    "        # Close pygame if renderer has been used\n",
    "        if self.render:\n",
    "            self.environment.render()\n",
    "\n",
    "        return cumul_rew, rewList, rewListCumul, obsList, actList "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On change la signature(_init_) <br> afin de pouvoir agir sur l'hyper paramètre epsilon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from example_submission import baseline_agents\n",
    "from example_submission import my_agents as ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Redéfinition de GreedySearch pour modifier epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypownet.agent\n",
    "import pypownet.environment\n",
    "import example_submission.preprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "class GreedySearch(pypownet.agent.Agent):\n",
    "    \"\"\" This agent is a tree-search model of depth 1, that is constrained to modifiying at most 1 substation\n",
    "    configuration or at most 1 line status. This controler used the simulate method of the environment, by testing\n",
    "    every 1-line status switch action, every new configuration for substations with at least 4 elements, as well as\n",
    "    the do-nothing action. Then, it will seek for the best reward and return the associated action, expecting\n",
    "    the maximum reward for the action pool it can reach.\n",
    "    Note that the simulate method is only an approximation of the step method of the environment, and in three ways:\n",
    "    * simulate uses the DC mode, while step is in AC\n",
    "    * simulate uses only the predictions given to the player to simulate the next timestep injections\n",
    "    * simulate can not compute the hazards that are supposed to come at the next timestep\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, environment,epsi):\n",
    "        super().__init__(environment)\n",
    "        self.verbose = False\n",
    "        self.epsilon = epsi\n",
    "        self.ioman = ActIOnManager(destination_path='saved_actions.csv')\n",
    "        self.ioman2 = ActIOnManager(destination_path='saved_states.csv')\n",
    "        self.ioman3  = ActIOnManager(destination_path='saved_rewards.csv')\n",
    "        random.seed()\n",
    "\n",
    "    def actGS(self, observation):\n",
    "        import itertools\n",
    "\n",
    "         # Sanity check: an observation is a structured object defined in the environment file.\n",
    "        assert isinstance(observation, pypownet.environment.Observation)\n",
    "        action_space = self.environment.action_space\n",
    "\n",
    "        number_lines = action_space.lines_status_subaction_length\n",
    "        # Will store reward, actions, and action name, then eventually pick the maximum reward and retrieve the\n",
    "        # associated values\n",
    "        rewards, actions, names = [], [], []\n",
    "\n",
    "        # Test doing nothing\n",
    "        if self.verbose:\n",
    "            print(' Simulation with no action', end='')\n",
    "        action = action_space.get_do_nothing_action()\n",
    "        reward_aslist = self.environment.simulate(action, do_sum=False)\n",
    "        reward = sum(reward_aslist)\n",
    "        if self.verbose:\n",
    "            print('; reward: [', ', '.join(['%.2f' % c for c in reward_aslist]), '] =', reward)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        names.append('no action')\n",
    "\n",
    "        # Test every line opening\n",
    "        for l in range(number_lines):\n",
    "            if self.verbose:\n",
    "                print(' Simulation with switching status of line %d' % l, end='')\n",
    "            action = action_space.get_do_nothing_action()\n",
    "            action_space.set_lines_status_switch_from_id(action=action, line_id=l, new_switch_value=1)\n",
    "            reward_aslist = self.environment.simulate(action, do_sum=False)\n",
    "            reward = sum(reward_aslist)\n",
    "            if self.verbose:\n",
    "                print('; reward: [', ', '.join(['%.2f' % c for c in reward_aslist]), '] =', reward)\n",
    "            rewards.append(reward)\n",
    "            actions.append(action)\n",
    "            names.append('switching status of line %d' % l)\n",
    "\n",
    "        # For every substation with at least 4 elements, try every possible configuration for the switches\n",
    "        for substation_id in action_space.substations_ids:\n",
    "            substation_n_elements = action_space.get_number_elements_of_substation(substation_id)\n",
    "            if 6 > substation_n_elements > 3:\n",
    "                # Look through all configurations of n_elements binary vector with first value fixed to 0\n",
    "                for configuration in list(itertools.product([0, 1], repeat=substation_n_elements - 1)):\n",
    "                    new_configuration = [0] + list(configuration)\n",
    "                    if self.verbose:\n",
    "                        print(' Simulation with change in topo of sub. %d with switches %s' % (\n",
    "                            substation_id, repr(new_configuration)), end='')\n",
    "                    # Construct action\n",
    "                    action = action_space.get_do_nothing_action()\n",
    "                    action_space.set_switches_configuration_of_substation(action=action,\n",
    "                                                                          substation_id=substation_id,\n",
    "                                                                          new_configuration=new_configuration)\n",
    "                    reward_aslist = self.environment.simulate(action, do_sum=False)\n",
    "                    reward = sum(reward_aslist)\n",
    "                    if self.verbose:\n",
    "                        print('; reward: [', ', '.join(['%.2f' % c for c in reward_aslist]), '] =', reward)\n",
    "                    rewards.append(reward)\n",
    "                    actions.append(action)\n",
    "                    names.append('change in topo of sub. %d with switches %s' % (substation_id,\n",
    "                                                                                 repr(new_configuration)))\n",
    "\n",
    "        # Take the best reward, and retrieve the corresponding action\n",
    "        best_reward = max(rewards)\n",
    "        best_index = rewards.index(best_reward)\n",
    "        best_action = actions[best_index]\n",
    "        best_action_name = names[best_index]\n",
    "\n",
    "        # Dump best action into stored actions file\n",
    "        self.ioman.dump(best_action)\n",
    "        self.ioman3.dumpReward(best_reward)\n",
    "        self.ioman2.dumpState(observation.as_array())\n",
    "\n",
    "        if self.verbose:\n",
    "            print('Action chosen: ', best_action_name, '; expected reward %.4f' % best_reward)\n",
    "\n",
    "        return best_action\n",
    "\n",
    "    def actRNS(self, observation):\n",
    "        # Sanity check: an observation is a structured object defined in the environment file.\n",
    "        assert isinstance(observation, pypownet.environment.Observation)\n",
    "        action_space = self.environment.action_space\n",
    "\n",
    "        # Create template of action with no switch activated (do-nothing action)\n",
    "        action = action_space.get_do_nothing_action()\n",
    "\n",
    "        # Select a random substation ID on which to perform node-splitting\n",
    "        target_substation_id = np.random.choice(action_space.substations_ids)\n",
    "        expected_target_configuration_size = action_space.get_number_elements_of_substation(target_substation_id)\n",
    "        # Choses a new switch configuration (binary array)\n",
    "        target_configuration = np.random.choice([0, 1], size=(expected_target_configuration_size,))\n",
    "\n",
    "        action_space.set_switches_configuration_of_substation(action=action,\n",
    "                                                              substation_id=target_substation_id,\n",
    "                                                              new_configuration=target_configuration)\n",
    "\n",
    "        # Ensure changes have been done on action\n",
    "        current_configuration, _ = action_space.get_switches_configuration_of_substation(action, target_substation_id)\n",
    "        assert np.all(current_configuration == target_configuration)\n",
    "\n",
    "        # Dump best action into stored actions file\n",
    "         #self.ioman.dump(action)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def actRLS(self, observation):\n",
    "        # Sanity check: an observation is a structured object defined in the environment file.\n",
    "        assert isinstance(observation, pypownet.environment.Observation)\n",
    "        action_space = self.environment.action_space\n",
    "\n",
    "        # Create template of action with no switch activated (do-nothing action)\n",
    "        action = action_space.get_do_nothing_action()\n",
    "\n",
    "        # Randomly switch one line\n",
    "        l = np.random.randint(action_space.lines_status_subaction_length)\n",
    "        action_space.set_lines_status_switch_from_id(action=action,\n",
    "                                                     line_id=l,\n",
    "                                                     new_switch_value=1)\n",
    "\n",
    "        # Test the reward on the environment\n",
    "        reward_aslist = self.environment.simulate(action, do_sum=False)\n",
    "        reward = sum(reward_aslist)\n",
    "        if self.verbose:\n",
    "            print('reward: [', ', '.join(['%.2f' % c for c in reward_aslist]), '] =', reward)\n",
    "\n",
    "        action_name = 'switching status of line %d' % l\n",
    "        if self.verbose:\n",
    "            print('Action chosen: ', action_name, '; expected reward %.4f' % reward)\n",
    "\n",
    "        return action\n",
    "\n",
    "        # No learning (i.e. self.feed_reward does pass)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def act(self, observation):\n",
    "        x = random.random()\n",
    "        if x<= self.epsilon:\n",
    "            return self.actRNS(observation)\n",
    "        elif x <= 2*self.epsilon:\n",
    "            return self.actRLS(observation)\n",
    "        else:\n",
    "            return self.actGS(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Agents supplémentaires d'imitation avec des classifieurs différents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "class ImitationAgentSVC(pypownet.agent.Agent):\n",
    "\n",
    "    def __init__(self, environment):\n",
    "        random.seed()\n",
    "        super().__init__(environment)\n",
    "        prepro = example_submission.preprocessing.Preprocessing(\"saved_actions.csv\",\"saved_states.csv\",\"saved_rewards.csv\")\n",
    "        self.data = prepro.main()\n",
    "        X = self.data[0]\n",
    "        y = self.data[1]\n",
    "        y_label = []\n",
    "        for i in range(len(y)):\n",
    "            y_label.append(self.compute_action_key(y[i]))\n",
    "        self.agent = SVC().fit(X, y_label)\n",
    "\n",
    "    def compute_action_key(self, array):\n",
    "        key =\"\"\n",
    "        for i in range(len(array)):\n",
    "            key = key + str(array[i])\n",
    "        return key\n",
    "\n",
    "    def decode_from_key(self, key):\n",
    "        action = np.zeros(len(key[0]))\n",
    "        for i in range(len(key[0])):\n",
    "            if key[0][i] == \"1\":\n",
    "                action[i] = 1\n",
    "        return action\n",
    "\n",
    "    def act(self, observation):\n",
    "        state = observation.as_array()\n",
    "        id_action = self.agent.predict([state])\n",
    "        action_space = self.environment.action_space\n",
    "        return action_space.array_to_action(self.decode_from_key(id_action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "class ImitationAgentGauss(pypownet.agent.Agent):\n",
    "\n",
    "    def __init__(self, environment):\n",
    "        random.seed()\n",
    "        super().__init__(environment)\n",
    "        prepro = example_submission.preprocessing.Preprocessing(\"saved_actions.csv\",\"saved_states.csv\",\"saved_rewards.csv\")\n",
    "        self.data = prepro.main()\n",
    "        X = self.data[0]\n",
    "        y = self.data[1]\n",
    "        y_label = []\n",
    "        for i in range(len(y)):\n",
    "            y_label.append(self.compute_action_key(y[i]))\n",
    "        self.agent = GaussianNB().fit(X, y_label)\n",
    "\n",
    "    def compute_action_key(self, array):\n",
    "        key =\"\"\n",
    "        for i in range(len(array)):\n",
    "            key = key + str(array[i])\n",
    "        return key\n",
    "\n",
    "    def decode_from_key(self, key):\n",
    "        action = np.zeros(len(key[0]))\n",
    "        for i in range(len(key[0])):\n",
    "            if key[0][i] == \"1\":\n",
    "                action[i] = 1\n",
    "        return action\n",
    "\n",
    "    def act(self, observation):\n",
    "        state = observation.as_array()\n",
    "        id_action = self.agent.predict([state])\n",
    "        action_space = self.environment.action_space\n",
    "        return action_space.array_to_action(self.decode_from_key(id_action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> On prépare l'environnement pour notre agent </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'example_submission/'\n",
    "problem_dir = 'ingestion_program/'  \n",
    "score_dir = 'scoring_program/'\n",
    "input_dir = 'public_data/'\n",
    "output_dir = 'output/'\n",
    "from sys import path; path.append(model_dir); path.append(problem_dir); path.append(score_dir);\n",
    "path.append(input_dir); path.append(output_dir);\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "# Uncomment the next lines to auto-reload libraries (this causes some problem with pickles in Python 3)\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')\n",
    "import seaborn as sns; sns.set()\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'public_data/hard'              # Change this to the directory where you put the input data\n",
    "get_ipython().system('ls $data_dir*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pypownet.environment\n",
    "import pypownet.runner\n",
    "data_dir = 'public_data'  \n",
    "environment = pypownet.environment.RunEnv(parameters_folder=os.path.abspath(data_dir),\n",
    "                                              game_level=\"hard\",\n",
    "                                              chronic_looping_mode='fixed', start_id=0,\n",
    "                                              game_over_mode=\"soft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chronics = 1000#8065 #Can set lower or higher but will bug on the association raw data & agent\n",
    "x = np.arange(0,all_chronics)\n",
    "NUMBER_ITERATIONS = all_chronics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring_program import libscores\n",
    "from libscores import get_metric\n",
    "metric_name, scoring_function = get_metric()\n",
    "print('Using scoring metric:', metric_name)\n",
    "# Uncomment the next line to display the code of the scoring metric\n",
    "#??scoring_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> On va lancer les différents agents et récupèrer les éléments intérressant à afficher</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> On lance l'agent DoNothing </h2> (temps estimé : quelques secondes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "submission_dir = 'example_submission'\n",
    "sys.path.append(submission_dir)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "log_path = os.path.abspath(os.path.join(output_dir, 'runner.log'))\n",
    "\n",
    "\n",
    "open(log_path, 'w').close()\n",
    "submitted_controler = ma.DoNothingAgent(environment)\n",
    "# Instanciate a runner, that will save the run statistics within the log_path file, to be parsed and processed\n",
    "# by the scoring program\n",
    "phase_runner = RunnerPerso(environment, submitted_controler, verbose=True, vverbose=False,\n",
    "                                      log_filepath=log_path)\n",
    "phase_runner.ch.setLevel(logging.ERROR)\n",
    "# Run the planned experiment of this phase with the submitted model\n",
    "score, rewListDN, rewListCumulDN,obsDN, actionDN = phase_runner.loop(iterations=NUMBER_ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> On lance l'agent Determinist</h2> (temps estimé : quelques secondes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import time\n",
    "\n",
    "submission_dir = 'example_submission'\n",
    "sys.path.append(submission_dir)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "log_path = os.path.abspath(os.path.join(output_dir, 'runner.log'))\n",
    "\n",
    "\n",
    "open(log_path, 'w').close()\n",
    "submitted_controler = ma.DeterministSubmission(environment)\n",
    "# Instanciate a runner, that will save the run statistics within the log_path file, to be parsed and processed\n",
    "# by the scoring program\n",
    "phase_runner = RunnerPerso(environment, submitted_controler, verbose=True, vverbose=False,\n",
    "                                      log_filepath=log_path)\n",
    "phase_runner.ch.setLevel(logging.ERROR)\n",
    "# Run the planned experiment of this phase with the submitted model\n",
    "score, rewListDS, rewListCumulDS, obsDS, actionDS = phase_runner.loop(iterations=NUMBER_ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> On lance l'agent GreedySubmission </h2> (temps estimé : 20 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import time\n",
    "\n",
    "submission_dir = 'example_submission'\n",
    "sys.path.append(submission_dir)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "log_path = os.path.abspath(os.path.join(output_dir, 'runner.log'))\n",
    "\n",
    "\n",
    "open(log_path, 'w').close()\n",
    "submitted_controler = ma.GreedySubmission(environment)\n",
    "# Instanciate a runner, that will save the run statistics within the log_path file, to be parsed and processed\n",
    "# by the scoring program\n",
    "phase_runner = RunnerPerso(environment, submitted_controler, verbose=True, vverbose=False,\n",
    "                                      log_filepath=log_path)\n",
    "phase_runner.ch.setLevel(logging.ERROR)\n",
    "# Run the planned experiment of this phase with the submitted model\n",
    "score, rewListGDS, rewListCumulGDS, obsGDS, actionGDS = phase_runner.loop(iterations=NUMBER_ITERATIONS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> On lance l'agent de QLearning </h2> (temps estimé : 2-3 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import time\n",
    "\n",
    "submission_dir = 'example_submission'\n",
    "sys.path.append(submission_dir)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "log_path = os.path.abspath(os.path.join(output_dir, 'runner.log'))\n",
    "\n",
    "open(log_path, 'w').close()\n",
    "submitted_controler = ma.QLearningAgent(environment)\n",
    "# Instanciate a runner, that will save the run statistics within the log_path file, to be parsed and processed\n",
    "# by the scoring program\n",
    "phase_runner = RunnerPerso(environment, submitted_controler, verbose=True, vverbose=False,\n",
    "                                      log_filepath=log_path)\n",
    "phase_runner.ch.setLevel(logging.ERROR)\n",
    "# Run the planned experiment of this phase with the submitted model\n",
    "score, rewListQL, rewListCumulQL, obsQL, actionQL = phase_runner.loop(iterations=NUMBER_ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> On lance l'agent d'Imitation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import time\n",
    "\n",
    "submission_dir = 'example_submission'\n",
    "sys.path.append(submission_dir)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "log_path = os.path.abspath(os.path.join(output_dir, 'runner.log'))\n",
    "\n",
    "open(log_path, 'w').close()\n",
    "submitted_controler = ma.ImitationAgent(environment)\n",
    "# Instanciate a runner, that will save the run statistics within the log_path file, to be parsed and processed\n",
    "# by the scoring program\n",
    "phase_runner = RunnerPerso(environment, submitted_controler, verbose=True, vverbose=False,\n",
    "                                      log_filepath=log_path)\n",
    "phase_runner.ch.setLevel(logging.ERROR)\n",
    "# Run the planned experiment of this phase with the submitted model\n",
    "score, rewListI, rewListCumulI, obsI,actI = phase_runner.loop(iterations=NUMBER_ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> On lance des GreedySearch avec epsilon variant pour comparer leurs performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "submission_dir = 'example_submission'\n",
    "sys.path.append(submission_dir)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "log_path = os.path.abspath(os.path.join(output_dir, 'runner.log'))\n",
    "\n",
    "\n",
    "open(log_path, 'w').close()\n",
    "submitted_controler = GreedySearch(environment, 0.4)\n",
    "# Instanciate a runner, that will save the run statistics within the log_path file, to be parsed and processed\n",
    "# by the scoring program\n",
    "phase_runner = RunnerPerso(environment, submitted_controler, verbose=True, vverbose=False,\n",
    "                                      log_filepath=log_path)\n",
    "phase_runner.ch.setLevel(logging.ERROR)\n",
    "\n",
    "# Run the planned experiment of this phase with the submitted model\n",
    "score, rewListGSE04, rewListCumulGSE04,obsGSE04, actionGSE04 = phase_runner.loop(iterations=NUMBER_ITERATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "submission_dir = 'example_submission'\n",
    "sys.path.append(submission_dir)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "log_path = os.path.abspath(os.path.join(output_dir, 'runner.log'))\n",
    "\n",
    "\n",
    "open(log_path, 'w').close()\n",
    "submitted_controler = GreedySearch(environment, 0.2)\n",
    "# Instanciate a runner, that will save the run statistics within the log_path file, to be parsed and processed\n",
    "# by the scoring program\n",
    "phase_runner = RunnerPerso(environment, submitted_controler, verbose=True, vverbose=False,\n",
    "                                      log_filepath=log_path)\n",
    "phase_runner.ch.setLevel(logging.ERROR)\n",
    "\n",
    "# Run the planned experiment of this phase with the submitted model\n",
    "score, rewListGSE02, rewListCumulGSE02,obsGSE02, actionGSE02 = phase_runner.loop(iterations=NUMBER_ITERATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "submission_dir = 'example_submission'\n",
    "sys.path.append(submission_dir)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "log_path = os.path.abspath(os.path.join(output_dir, 'runner.log'))\n",
    "\n",
    "\n",
    "open(log_path, 'w').close()\n",
    "submitted_controler = GreedySearch(environment, 0.1)\n",
    "# Instanciate a runner, that will save the run statistics within the log_path file, to be parsed and processed\n",
    "# by the scoring program\n",
    "phase_runner = RunnerPerso(environment, submitted_controler, verbose=True, vverbose=False,\n",
    "                                      log_filepath=log_path)\n",
    "phase_runner.ch.setLevel(logging.ERROR)\n",
    "\n",
    "# Run the planned experiment of this phase with the submitted model\n",
    "score, rewListGSE01, rewListCumulGSE01,obsGSE01, actionGSE01 = phase_runner.loop(iterations=NUMBER_ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> On lance nos agents d'imitation supplémentaires pour comparer leurs performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import time\n",
    "\n",
    "submission_dir = 'example_submission'\n",
    "sys.path.append(submission_dir)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "log_path = os.path.abspath(os.path.join(output_dir, 'runner.log'))\n",
    "\n",
    "open(log_path, 'w').close()\n",
    "submitted_controler = ImitationAgentSVC(environment)\n",
    "# Instanciate a runner, that will save the run statistics within the log_path file, to be parsed and processed\n",
    "# by the scoring program\n",
    "phase_runner = RunnerPerso(environment, submitted_controler, verbose=True, vverbose=False,\n",
    "                                      log_filepath=log_path)\n",
    "phase_runner.ch.setLevel(logging.ERROR)\n",
    "# Run the planned experiment of this phase with the submitted model\n",
    "score, rewListISVC, rewListCumulISVC, obsISVC,actISVC = phase_runner.loop(iterations=NUMBER_ITERATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import time\n",
    "\n",
    "submission_dir = 'example_submission'\n",
    "sys.path.append(submission_dir)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "log_path = os.path.abspath(os.path.join(output_dir, 'runner.log'))\n",
    "\n",
    "open(log_path, 'w').close()\n",
    "submitted_controler = ImitationAgentGauss(environment)\n",
    "# Instanciate a runner, that will save the run statistics within the log_path file, to be parsed and processed\n",
    "# by the scoring program\n",
    "phase_runner = RunnerPerso(environment, submitted_controler, verbose=True, vverbose=False,\n",
    "                                      log_filepath=log_path)\n",
    "phase_runner.ch.setLevel(logging.ERROR)\n",
    "# Run the planned experiment of this phase with the submitted model\n",
    "score, rewListINB, rewListCumulINB, obsINB,actINB = phase_runner.loop(iterations=NUMBER_ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Création de tous les graphiques </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Comparasion des différents agents selon le reward by iterations ou le reward cumulé </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = (13,13))\n",
    "ax.plot(x,rewListCumulDN, label = \"DoNothing\", Color = 'r')\n",
    "ax.plot(x,rewListCumulI, label = \"Imitation\", Color = 'b')\n",
    "ax.plot(x,rewListCumulGDS, label = \"Determinist\", Color = 'y')\n",
    "ax.plot(x,rewListCumulQL, label = \"QLearning\", Color = 'p')\n",
    "ax.plot(x,rewListCumulGS, label = \"GreedySubmission\", Color = 'g')\n",
    "ax.set_title(\"Comparaison DoNothing - Second Submission rewards by iteration\")\n",
    "ax.set_xlabel('Iteration n°')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Affichage de des résultats de GreedySearch en fonction de ses hyper Paramètres </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize = (25,16))\n",
    "ax.plot(x,rewListCumulGSE01, label = \"Epsilon = 0.1\", Color = 'r')\n",
    "ax.plot(x,rewListCumulGSE02, label = \"Epsilon = 0.2\", Color = 'y')\n",
    "ax.plot(x,rewListCumulGSE04, label = \"Epsilon = 0.4\", Color = 'm')\n",
    "ax.set_title(\"Comparaison GreedySearch en fonction des HyperParamètres\", fontsize =30)\n",
    "ax.set_xlabel('Iteration n°',fontsize =30)\n",
    "ax.set_ylabel('Reward Cumulé',fontsize =30)\n",
    "ax.legend(fontsize =30, loc = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Résultats d'Imitation en fonction des méthodes de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize = (25,16))\n",
    "ax.plot(x,rewListCumulI, label = \"MLPClassifier\", Color = 'r')\n",
    "ax.plot(x,rewListCumulISVC, label = \"SVC\", Color = 'g')\n",
    "ax.plot(x,rewListCumulINB, label = \"GaussianNB\", Color = 'b')\n",
    "ax.set_title(\"Comparaison d'Imitation Agent selon le classifieur\", fontsize =30)\n",
    "ax.set_xlabel('Iteration n°',fontsize =30)\n",
    "ax.set_ylabel('Reward Cumulé',fontsize =30)\n",
    "ax.legend(fontsize =30, loc = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Affichage simple des Reward pour un unique agent </h2> (Ajouter les initiales de l'agent) <br>\n",
    "<ul> \n",
    "    <li> GreedySearch : GSE0(epsilon_decimal_value) </li>\n",
    "    <li> DoNothing : DN </li>\n",
    "    <li> DeterministSubmission : DS </li>\n",
    "    <li> GreedySubmission : GDS</li>\n",
    "    <li> QLearning : QL </li>\n",
    "    <li> Imitation : I</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(x,rewList#REPLACEHERE#)\n",
    "ax.set_title('DoNothing rewards by iteration')\n",
    "ax.set_xlabel('Iteration n°')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Méthode pour réduire les valeurs extrème (Game Over) des rewards </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt = 0\n",
    "for i in rewList#REPLACEHERE# :\n",
    "    if i ==-2:\n",
    "        rewList[cpt] =-2\n",
    "    cpt+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Préparation des données Raw pour l'affichage </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "data_dir = 'public_data/hard'              # Change this to the directory where you put the input data\n",
    "!ls $data_dir*\n",
    "loads_p_file = '_N_loads_p.csv'\n",
    "prods_p_file = '_N_prods_p.csv'\n",
    "datetimes_file = '_N_datetimes.csv'\n",
    "maintenance_file = 'maintenance.csv'\n",
    "hazards_file = 'hazards.csv'\n",
    "imaps_file = '_N_imaps.csv'\n",
    "df_loads_p = {}\n",
    "df_prods_p = {}\n",
    "df_datetimes = {}\n",
    "df_maintenance = {}\n",
    "df_hazards = {} \n",
    "i = 0 # chronics id\n",
    "\n",
    "path = data_dir+'/chronics/'+str(i)+'/'\n",
    "df_loads_p[str(i)] = pd.read_csv(path+loads_p_file, sep=';')\n",
    "df_prods_p[str(i)] = pd.read_csv(path+prods_p_file, sep=';')\n",
    "df_datetimes[str(i)] = pd.read_csv(path+datetimes_file, sep=';')\n",
    "df_loads_p[str(i)].index = pd.to_datetime(df_datetimes[str(i)]['date'] + ' ' + df_datetimes[str(i)]['time'])\n",
    "df_prods_p[str(i)].index = pd.to_datetime(df_datetimes[str(i)]['date'] + ' ' + df_datetimes[str(i)]['time'])\n",
    "df_maintenance[str(i)] = pd.read_csv(path+maintenance_file, sep=';')\n",
    "df_maintenance[str(i)].index = pd.to_datetime(df_datetimes[str(i)]['date'] + ' ' + df_datetimes[str(i)]['time'])\n",
    "df_hazards[str(i)] = pd.read_csv(path+hazards_file, sep=';')\n",
    "df_hazards[str(i)].index = pd.to_datetime(df_datetimes[str(i)]['date'] + ' ' + df_datetimes[str(i)]['time'])\n",
    "df_imaps = pd.read_csv(path + imaps_file, sep=';')\n",
    "\n",
    "total_prod =[0]*8065\n",
    "for i in df_loads_p['0']:\n",
    "    total_prod+=df_loads_p['0'][i]\n",
    "total_consuption = [0]*8065\n",
    "for i in df_prods_p['0']:\n",
    "    total_consuption += df_prods_p['0'][i]\n",
    "hazards = [0]*8065\n",
    "for i in df_hazards['0']:\n",
    "    hazards += df_hazards['0'][i]\n",
    "maintenance = [0]*8065\n",
    "for i in df_maintenance['0']:\n",
    "    maintenance += df_maintenance['0'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Affichage des données Raw </h2>\n",
    "<h3> Affichage des productions </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,4])\n",
    "plt.title('Prods Time Series #'+str(0))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Power (MW)')\n",
    "plt.plot(df_prods_p[str(0)])\n",
    "plt.legend(list(df_prods_p[str(0)]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Comparaison consommation/Production </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1,8066)\n",
    "fig, ax = plt.subplots(1,1,figsize = (20,13))\n",
    "ax.plot(x,total_consuption, label = \"Consuption\", Color = 'r')\n",
    "ax.plot(x,total_prod, label = \"Prod\", Color = 'g')\n",
    "ax.set_title(\"Consuption VS Production\")\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Power (MW)')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Mise en superposition du reward de l'agent avec la consommation et les hazards </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1,figsize = (20,18))\n",
    "coefDiv = 1\n",
    "decalage = 0\n",
    "ax[0].plot(x[decalage:int(len(x)/coefDiv)+decalage],rewListDN[decalage:int(len(rewListDN)/coefDiv)+decalage], label = \"DoNothing\", Color = 'r')\n",
    "ax[0].plot(x[decalage:int(len(x)/coefDiv)+decalage],rewListGSE01[decalage:int(len(rewListGSE01)/coefDiv)+decalage], label = \"GreedySearch Epsi = 0.1\", Color = 'b')\n",
    "ax[0].plot(x[decalage:int(len(x)/coefDiv)+decalage],rewListI[decalage:int(len(rewListI)/coefDiv)+decalage], label = \"Imitation\", Color = 'g')\n",
    "ax[0].set_title(\"Comparaison DoNothing - First Submission rewards by iteration\")\n",
    "ax[0].set_xlabel('Iteration n°')\n",
    "ax[0].set_ylabel('Reward')\n",
    "ax[0].legend()\n",
    "ax[1].plot(x[decalage:int(len(x)/coefDiv)+decalage],total_consuption[decalage:int(len(total_consuption)/coefDiv)+decalage], label = \"Consuption\", Color = 'r')\n",
    "ax[1].plot(x[decalage:int(len(x)/coefDiv)+decalage],total_prod[decalage:int(len(total_prod)/coefDiv)+decalage], label = \"Prod\", Color = 'g')\n",
    "ax[1].set_title(\"Consuption VS Production\")\n",
    "ax[1].set_xlabel('Time')\n",
    "ax[1].set_ylabel('Power (MW)')\n",
    "ax[1].legend()\n",
    "ax[2].plot(x[decalage:int(len(x)/coefDiv)+decalage],hazards[decalage:int(len(total_consuption)/coefDiv)+decalage], label = \"Hazards\", Color = 'y')\n",
    "ax[2].plot(x[decalage:int(len(x)/coefDiv)+decalage],hazards[decalage:int(len(total_consuption)/coefDiv)+decalage], label = \"Maintenace\", Color = 'b')\n",
    "ax[2].set_title(\"Consuption VS Production\")\n",
    "ax[2].set_xlabel('Time')\n",
    "ax[2].set_ylabel('Power (MW)')\n",
    "ax[2].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Préparation des données thermique <br> et de la charge des substations en fonction de l'agent X</h2>\n",
    "(changer obsX avec le nom de l'agent souhaité)<br>\n",
    "<ul>\n",
    "    <li> obsDeterminist </li>\n",
    "    <li> obsGreedy </li>\n",
    "    <li> obsQLearn </li>\n",
    "    <li> obsImitation </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy as cp\n",
    "obsDeterminist = cp.deepcopy(obsDS)\n",
    "obsGreedy = cp.deepcopy(obsGDS)\n",
    "obsQLearn = cp.deepcopy(obsQL)\n",
    "obsImitation = cp.deepcopy(obsI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsX = cp.deepcopy(obs#NAMEOFAGENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampere = []\n",
    "nbNodes = []\n",
    "for i in obsX:\n",
    "    ampere.append(i.ampere_flows)\n",
    "    nbNodes.append(i.loads_nodes)\n",
    "ampere = np.array(ampere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Affichage des données thermique et de la charge des substations  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(20,15))\n",
    "ax[0].plot(x,ampere[:,0],color='r',label='Ligne 0')\n",
    "ax[0].plot(x,ampere[:,1],color='g',label='Ligne 1')\n",
    "ax[0].plot(x,ampere[:,2],color='y',label='Ligne 2')\n",
    "ax[0].plot(x,ampere[:,3],color='b',label='Ligne 3')\n",
    "ax[0].plot(x,ampere[:,4],color='m',label='Ligne 4')\n",
    "ax[0].plot(x,ampere[:,5],color='k',label='Ligne 5')\n",
    "ax[0].plot(x,ampere[:,6],color='c',label='Ligne 6')\n",
    "ax[1].plot(x,nbNodes)\n",
    "#ax.set_title(\"Second Submission rewards by iteration\")\n",
    "#ax.set_xlabel('Iteration n°')\n",
    "#ax.set_ylabel('Reward')\n",
    "#ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Préparation des données des actions en fonction de l'agent X </h2>\n",
    "(changer obsX avec le nom de l'agent souhaité)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsX = cp.deepcopy(obs#agentName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cpt_switch_or = np.zeros(20)\n",
    "cpt_switch_on = np.zeros(20)\n",
    "cpt_switch_ex = np.zeros(20)\n",
    "\n",
    "cpt_loads_cut = np.zeros(len(obsX[0].are_loads_cut))\n",
    "cpt_prods_cut = np.zeros(len(obsX[0].are_productions_cut))\n",
    "\n",
    "#Methode de recuperation du nombre de switch de ligne ou de prods ou loads\n",
    "for i in range(1,len(obsX)): \n",
    "    for j in range(0,20):\n",
    "        if (obsX[i].lines_or_nodes[j] != obsX[i-1].lines_or_nodes[j]):\n",
    "            cpt_switch_or[j]+=1\n",
    "        if (obsX[i].lines_status[j] != obsX[i-1].lines_status[j]):\n",
    "            cpt_switch_on[j]+=1\n",
    "        if (obsX[i].lines_ex_nodes[j] != obsX[i-1].lines_ex_nodes[j]):\n",
    "            cpt_switch_ex[j]+=1\n",
    "    cpt = 0\n",
    "    for j in obsX[i].are_loads_cut:\n",
    "        if j :\n",
    "            cpt_loads_cut[cpt]+=1\n",
    "        cpt+=1\n",
    "    cpt = 0\n",
    "    for j in obsX[i].are_productions_cut:\n",
    "        if j :\n",
    "            cpt_prods_cut[cpt]+=1\n",
    "        cpt+=1\n",
    "\n",
    "lines_switch_or = []\n",
    "lines_switch_on = []\n",
    "lines_switch_ex = []\n",
    "\n",
    "loads_cut = []\n",
    "prods_cut = []\n",
    "\n",
    "#Methode pour ranger le nombre de switch pour faire chaque lignes ou prods ou loads\n",
    "for i in range(20):\n",
    "    if int(cpt_switch_or[i]) > 0:\n",
    "        lines_switch_or.append([i])\n",
    "        for j in range(int(cpt_switch_or[i])):\n",
    "            lines_switch_or[-1].append(i)\n",
    "    if int(cpt_switch_on[i]) > 0:\n",
    "        lines_switch_on.append([])\n",
    "        for j in range(int(cpt_switch_on[i])):\n",
    "            lines_switch_on[-1].append(i)\n",
    "    if int(cpt_switch_ex[i]) > 0:\n",
    "        lines_switch_ex.append([])\n",
    "        for j in range(int(cpt_switch_ex[i])):\n",
    "            lines_switch_ex[-1].append(i)\n",
    "for i in range(11):\n",
    "    if int(cpt_loads_cut[i]) > 0:\n",
    "        loads_cut.append([])\n",
    "        for j in range(int(cpt_loads_cut[i])):\n",
    "            loads_cut[-1].append(i)\n",
    "for i in range(5):\n",
    "    if int(cpt_prods_cut[i]) > 0:\n",
    "        prods_cut.append([])\n",
    "        for j in range(int(cpt_prods_cut[i])):\n",
    "            prods_cut[-1].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Affichage des actions subies par des lignes ou substations </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,1,figsize=(20,75))\n",
    "ax[0].hist(lines_switch_or, bins=range(1,21),rwidth=1,hatch='X',align='right')\n",
    "ax[0].set_title(\"Nombre de switch d'origne pour chaque ligne\")\n",
    "ax[0].set_xlabel('Ligne n°')\n",
    "ax[0].set_ylabel('Nombre de switch')\n",
    "#ax[0].legend()\n",
    "ax[1].hist(lines_switch_on)\n",
    "ax[1].set_title(\"Nombre de switch on/off pour chaque ligne\")\n",
    "ax[1].set_xlabel('Ligne n°')\n",
    "ax[1].set_ylabel('Nombre de switch')\n",
    "ax[2].hist(lines_switch_ex)\n",
    "ax[2].set_title(\"Nombre de switch de fin pour chaque ligne\")\n",
    "ax[2].set_xlabel('Ligne n°')\n",
    "ax[2].set_ylabel('Nombre de switch')\n",
    "ax[3].hist(loads_cut)\n",
    "ax[3].set_title(\"Nombre de mise à l'écart pour chaque loads\")\n",
    "ax[3].set_xlabel('Loads n°')\n",
    "ax[3].set_ylabel(\"Nombre de mise à l'ecart\")\n",
    "ax[4].hist(prods_cut,bins=range(5),align='mid')\n",
    "ax[4].set_title(\"Nombre de mise à l'ecart pour chaque prods\")\n",
    "ax[4].set_xlabel('Prods n°')\n",
    "ax[4].set_ylabel(\"Nombre de mise à l'ecart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Actions prises par notre Super Agent (Greedy Search) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import example_submission.preprocessing as pp\n",
    "prepro = pp.Preprocessing(\"saved_actions.csv\",\"saved_states.csv\",\"saved_rewards.csv\")\n",
    "actionDict = prepro.reduce_actions()\n",
    "actionList = list(actionDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "intToAct = dict()\n",
    "for i in range(len(actionList)):\n",
    "    intToAct[i] = actionList[i]\n",
    "\n",
    "actToInt = dict()\n",
    "for i in range(len(actionList)):\n",
    "    actToInt[actionList[i]] = i\n",
    "\n",
    "def codeAction(action):\n",
    "    return actToInt[action]\n",
    "\n",
    "def decodeAction(key):\n",
    "    return intToAct[key]\n",
    "\n",
    "histArray = []\n",
    "with open(\"saved_actions.csv\",'r') as doneActions:\n",
    "    lineN = 0\n",
    "    while lineN < 10001:\n",
    "        action = doneActions.readline()\n",
    "        action = action.replace(\",\",\"\")\n",
    "        action = action.replace(\"\\n\",\"\")\n",
    "        histArray.append(codeAction(action))\n",
    "        lineN += 1\n",
    "\n",
    "plt.figure(figsize=(13,7))\n",
    "plt.xticks(np.arange(0,54,2),fontsize=12)\n",
    "fig = plt.hist(histArray,np.arange(55))\n",
    "plt.xlabel(\"ID de l'action\",fontsize=20)\n",
    "plt.ylabel(\"Nombre d'états de meilleure action x\",fontsize=16)\n",
    "plt.title(\"Répartition des meilleures actions dans les états sur 10000 itérations\",fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Pour décoder l'action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decodeAction(#Numero))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
